---
title: "<u>ISBS Online Symposium</u>: Preparing Biomechanical Time Series' for Functional Data Analysis"
output: 
      github_document:
        toc: true
---

<div style="text-align: center;">
  <img src="isbs-logo.png" alt="ISBS Logo" width="200" style="margin-right: 50px;">
  <img src="fda-logo.png" alt="FDA Logo" width="200">
</div>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.asp = 1, fig.width = 5, fig.height = 5)
```

## Introduction

Welcome to this workshop on Functional Data Analysis (FDA) for Sports Biomechanics, as part of the ISBS Online Symposium. This document serves as a guide for working with time-series data and preparing it for functional data analysis in R.

* This tutorial <u>**does**</u> cover, basic data import, formatting, preparation and inspecting techniques. It will teach you how to go from raw biomechanical time series' to functional data that are ready to be analysed.

* This tutorial <u>**does not**</u> cover more advanced analytical techniques from FDA -- we have [a book](https://link.springer.com/book/10.1007/978-3-031-68862-1) and [material from a full one-day course](https://github.com/edwardgunning/ISBS-Short-Course) for this.

* We encourage participants to ask questions and discuss how the material relates to their own work, either during the presentation by using the "raise hand" function ion zoom, in the chat, or during the dedicated question time at the end.

* The material will remain **live and publicly available** after the workshop, in the hope that you will use it as part of your own data analysis and research.

* You can contact me at [Edward.Gunning@pennmedicine.upenn.edu](mailto:Edward.Gunning@pennmedicine.upenn.edu) if you spot any typos that should be fixed, or have any questions about the material.

## üñ• Computing Pre-requisites

### R and RStudio

If you want to follow along and program yourself, either retrospectively or in real time, you should have the following software installed:

* **The R Language for Statistical Computing**
  * It can be downloaded from [https://cloud.r-project.org](https://cloud.r-project.org)
  * For further assistance see [this video by RStudio education](https://vimeo.com/203516510)

* **The RStudio Integrated Development Environment (IDE)**
  * It can be downloaded from [https://posit.co/](https://posit.co/)
  * For further assistance see [this video by RStudio education](https://vimeo.com/203516510) (**Note**: The RStudio company has changed to Posit PBC, so there may be some minor differences)


**Note**: If you are unable to install R and RStudio, you can work with a free, lite web version of RStudio called [*posit cloud*](https://posit.cloud/). Watch [this video from Posit PBC](https://www.youtube.com/watch?v=-fzwm4ZhVQQ) to set up an account and get started.

We also recommend setting up an RStudio project to work and store your files for this workshop in -- see [this helpful guide on setting up projects by Posit PBC](https://support.posit.co/hc/en-us/articles/200526207-Using-RStudio-Projects).

**IMPORTANT**: **We do not require any previous knowledge of R programming or FDA**. We have structured the lecture and practical sessions in such a way that all levels of experience will be catered for. However, if interested, our favourite (free!) resources for getting up to speed with R are:

* [R for Data Science (2nd Edition)](https://r4ds.hadley.nz/) by Hadley Wickham, Mine √áetinkaya-Rundel, and Garrett Grolemund.

* [R Programming for Data Science](https://bookdown.org/rdpeng/rprogdatascience/) by Roger D. Peng.


# ‚è±Ô∏è Schedule

These times are in British Standard Time (BST):

| Time | Topic | Lead |
|-------:|:------|:---------:|
| $12.00$ - $12.25$ | Welcome, Introduction and Background | DH | 
| $12.25$ - $13.30$ | Practical Tutorial | EG |
| $13.30$ - $14.00$ | Q&A and Discussion | DH, EG & JW |

---
---
---


# üéí Main Tutorial

### Reading Time-Series Data

<details>

<summary>üñ±Ô∏è Click to Expand üñ± </summary>

In this workshop, we will use the GaitRec dataset, specifically the right leg vertical ground reaction force (vGRF) data.
The data are stored in a comma separated value (CSV) file on the [GaitRec Figshare link](https://figshare.com/articles/dataset/GRF_F_V_RAW_right/11394825?backTo=%2Fcollections%2FGaitRec_A_large-scale_ground_reaction_force_dataset_of_healthy_and_impaired_gait%2F4788012&file=22063200) and can be downloaded directly [here](https://figshare.com/ndownloader/files/22063200).
Each time series is stored as a row of the CSV file, with associated metadata including subject ID, session number, and trial ID. Since trials have different durations, some rows contain missing values (`NA`s) where the recorded data length varies. Our goal is to preprocess these data, preparing them for functional data analysis.

### Read in the csv file:

There are many options to read in text files in R (e.g., comma or tab separated values files). Here, we'll use base R `read.csv()` function.
Since we are working within a project, we just have to point to the *relative path* to the file.

```{r}
GRF_data <- read.csv(file = "data/GRF_F_V_RAW_right.csv")
```

---

***Aside:***

There exist faster functions in specialized packages for reading in large csv files. Let's time trial `read.csv()` with `read_csv()` from the `readr` package and `fread` from the `data.table` package. From just one run it seems `fread()` is marginally faster than `read_csv()`, but both are orders of magnitude faster than `read.csv()`.


```{r}
runtime_1 <- system.time(GRF_data_1 <- read.csv(file = "data/GRF_F_V_RAW_right.csv"))
runtime_2 <- system.time(GRF_data_2 <- readr::read_csv(file = "data/GRF_F_V_RAW_right.csv", 
                                                    col_types = c(rep("i", 3), rep("n", 405))))
runtime_3 <- system.time(GRF_data_3 <- data.table::fread(file = "data/GRF_F_V_RAW_right.csv"))

# Examine the results:
print(paste0("read.csv = ", round(runtime_1["elapsed"], 2), " seconds; read_csv = ",
             round(runtime_2["elapsed"], 2), " seconds; fread = ",
             round(runtime_3["elapsed"], 2), " seconds"))
rm(list = paste0("GRF_data_", 1:3)) # remove the created data objects from our environments
```

* We should actually do this comparison by running the code for multiple repetitions using `microbenchmark()` from the `microbenchmark` package.

</details>

---

### Inspecting the loaded data

<details>

<summary>üñ±Ô∏è Click to Expand üñ± </summary>

The data is read in as a data frame, which is useful for storing tabular data where the columns are heterogenous in nature (e.g., contains some numeric and some factors or strings).

```{r}
class(GRF_data)
```

This dataset contains over $75,000$ rows. This meaans its a fantastic resource for statistics and machine learning applications. However, for the purpose of this tutorial, we'll take a random sample of $200$ rows to make it more manageable.

```{r}
dim(GRF_data) # check dimensions
set.seed(1)
sample_inds <- sample(seq_len(length.out = nrow(GRF_data)), size = 200)
GRF_data <- GRF_data[sample_inds, ]
dim(GRF_data) # check dimensions again
```

We can also split the data out into the first three columns (subject, session and trial IDs) and the remaining $405$ columns which include the sampled time series.
Since the remaining $405$ columns are all numeric containing time series values, it is appropriate to store them as a matrix.

```{r}
meta_df <- GRF_data[, 1:3] # first three columns
GRF_matrix <- as.matrix(GRF_data[, - c(1:3)])  # remaining columns
```

We can also create a $405$-dimensional vector representing the time argument.
Since these data are sampled at $250$ Hz, we have that each time difference is $1/250$.

```{r}
frames_per_second <- 250
seconds_per_frame <- 1 / frames_per_second
time_seq <- seq(0, seconds_per_frame * (405 - 1), by = seconds_per_frame)
```

We can use the `matplot()` function to plot the columns of a matrix, so we need to transpose (rotate) `GRF_matrix` when passing it as an argument using the `t()` function.

```{r}
matplot(x = time_seq,
        y = t(GRF_matrix), 
        type = "b", 
        cex = 0.5, 
        pch = 20, 
        ylab = "vGRF",
        xlab = "time (seconds)")
```

</details>

## Data Preprocessing and Preparation

<details>

<summary>üñ±Ô∏è Click to Expand üñ± </summary>

In this section, we'll discuss a number of important issues in data preparation.

For this, we'll use the `fda` package so we need to load it.

```{r, message = FALSE, warning = FALSE}
library(fda)
```

</details>

### Issue 1: Smoothing 

<details>

<summary>üñ±Ô∏è Click to Expand üñ± </summary>

* The first issue is representing each sampled time series as a *smooth* function (or curve).

* In the`fda` package, this representation is done using a **basis function representation**.

* In short, we use a linear combination (or weighted sum) of some set of basis functions that *we know*, to approximate each individual curve. Then, under the hood, the data are stored as the combination of the basis (i.e., the known functions) and vector of basis coefficients for an individual curve (i.e., the weights).

* We can choose the basis coefficients based on whether we want to **smooth** or interpolate the raw data.


#### Demonstration: Representing a single curve

We'll extract the first row of the dataset. We only take the non `NA` values, and take the correponding values for the time argument.

```{r}
GRF_obs_full <- GRF_matrix[1,]
GRF_obs <- GRF_obs_full[!is.na(GRF_obs_full)]
time_seq_obs <- time_seq[seq_len(length(GRF_obs))]
```


```{r, fig.asp = 0.5, fig.width = 12}
Bspline_basis_k50 <- create.bspline.basis(rangeval = range(time_seq_obs),
                                           nbasis = 50)

GRF_obs_1_fdSmooth <- smooth.basis(argvals = time_seq_obs,
                                   y = GRF_obs, 
                                   fdParobj = Bspline_basis_k50)

par(mfrow = c(1, 2))
plot(Bspline_basis_k50, knots = FALSE, lty = 1)
abline(h = 1, lwd = 1.5)
title("BSpline Basis")
plot.fd(fd(coef = diag(GRF_obs_1_fdSmooth$fd$coefs[,1]),
                  Bspline_basis_k50), lty = 1, ylim = c(0, 800))
points(x = time_seq_obs, GRF_obs, pch = 20, col = "grey")
lines.fdSmooth(GRF_obs_1_fdSmooth, col = "black", lwd = 1.5)
title("Weighted BSpline Basis to Represent Curve")
```


**What is happening under the hood?** 

  $\rightarrow$ We have gone from a vector of discrete values to a representation of the functional data in terms of a vector of basis coefficients and a set of basis functions.
  
  These are stored as an `fd` object:
  
```{r}
GRF_obs_1_fd <- GRF_obs_1_fdSmooth$fd
GRF_obs_1_fd[["coefs"]]
GRF_obs_1_fd[["basis"]]
```

This means that we are not bound by the dicrete points at which the data are measured, we can evaluate the curve at **any** point in the domain, by simply evluating the basis functions at these points.


The first issue that needs to be considered -- **do we need to smooth or interpolate the data**?

* If they have been filtered already, e.g., using a Butterworth filter applied to marker trajectories, we probably don't need to re-smooth them. Therefore we choose a large number of basis functions `nbasis` and do not apply a smoothing penalty.


* If we have the raw signal and need to remove noise/ errors, we should use the FDA tools to smooth the data. We can either do this by choosing a smaller number of basis functions, or preferably, add a smoothness penalty controlled by a weight parameter $\lambda$ in the objective function when computing the coefficients. 


```{r, fig.asp = 0.85, fig.width = 7}
lambda_seq <- 10^seq(-10, 0, by = 1)

plot(x = time_seq_obs, GRF_obs, pch = 20, col = "grey")
for(lami in seq_along(lambda_seq)) {
  fdsmooth_i <- smooth.basis(argvals = time_seq_obs,
                             y = GRF_obs, 
                             fdParobj = fdPar(fdobj = Bspline_basis_k50,
                                              Lfdobj = 2,
                                              lambda = lambda_seq[lami]))
  lines.fdSmooth(fdsmooth_i, col = lami + 1)
}
legend("bottom", paste0("lambda = ", lambda_seq), 
       col = seq_along(lambda_seq) + 1, 
       lty = 1)
```

* We can see that varying $\lambda$ gives different fits, we should choose the optimal value by cross-validation or generalized cross-validation (see Chapter 2 of book).

* Refer to the book Chapter 2 for details on different types of bases and other considerations when smoothing (e.g., suitable representations of derivatives/ rates of change).

</details>

### Issue 2: Time Normalization

<details>

<summary>üñ±Ô∏è Click to Expand üñ± </summary>

* We have fitted to a single curve, and we have used the observed time values to define its domain.

* However, standard functional data analysis software (i.e., the `fda` package) requires the functional data to be represented on a common domain (**Note:** There are some exceptions such as the work of Gellar et al. (2014) and Sangalli et al. (2010) do not require a common domain/ equal length curves). 

* These data -- and many biomechanical time series -- are of different lengths because people take different amounts of time to complete a movement.

* Therefore we need to consider methods to standardize/ normalize the domains of each observation to use them with the `fda` package.

* The most common approach, particularly for periodic data or data with very clearly defined start and end points, is to linearly time (or length) normalise. That is, we linearly stretch or compress the time values so that the start and end of each curve match, and we can treat them as data defined on a common domain.


#### 2 (a) <u>Common Approach</u>: Resample to Time Normalize and then Smooth

* Use a cubic spline to interpolate the data to $101$ points (often done in motion capture software), Do this to each row of the data.

* We use `apply()` to do the same operation to each row of the data -- we could also do it in a loop.

* This gives us a $200 \times 101$ matrix that we can pass to `smooth.basis()` to smooth/ represent all the curves collectively.


```{r}
GRF_matrix_normalized <- apply(X = GRF_matrix, MARGIN = 1, 
                               FUN = function(x) {
                                 obs <- x[!is.na(x)]
                                 time_seq_obs <- time_seq[seq_len(length(obs))]
                                  spline(x = time_seq_obs, y = obs, n = 101)$y
                               })
GRF_matrix_normalized <- t(GRF_matrix_normalized)
time_seq_normalized <- 0:100
matplot(x = time_seq_normalized,
        y = t(GRF_matrix_normalized), 
        type = "b", 
        cex = 0.5, 
        pch = 20, 
        ylab = "vGRF",
        xlab = "time (seconds)")
```

* We can pass the $101\times200$ matrix (after transposing) to `smooth.basis()` and it will do the smoothig/ representation of all the curves collectively.

```{r}
Bspline_basis_k50_normalized <- create.bspline.basis(rangeval = c(0, 100), 
                                                     nbasis = 50)
GRF_fdSmooth <- smooth.basis(argvals = time_seq_normalized,
                             y = t(GRF_matrix_normalized), 
                             fdParobj = Bspline_basis_k50_normalized)
GRF_fd <- GRF_fdSmooth$fd 
plot(GRF_fd)
```

#### 2 (b) <u>More Principled Approach</u>: Do Smoothing and Time Normalization Together

* When we use cubic spline interpolation, we actually just interpolate onto a B-spline basis and evaluate at $101$ points.

* We can actually do all of this using the `fda` package, and this might be preferable for a few reasons:

  1.  We avoid an intermediate resampling step, which may distort/ lose information (though maybe not if the data are smooth enough and the grid of time points is fine enough)
  2. If the data *are* noisy, we don't really want to interpolate noise and smooth the result, we would prefer to smooth the resulting data.
  
Below is code that uses a very simple for loop to for each curve individually to a basis on the same domain and store the resulting `fd` object:

**Note:** We are using the same basis, but using different evaluation points to fit each curve, storing the coefficients and combining them to create the fd object.

```{r, fig.asp=0.5, fig.width=10}
coef_matrix <- matrix(NA, nrow = 200, ncol = Bspline_basis_k50_normalized$nbasis)
for(i in 1:200) {
  obs_i <- GRF_matrix[i, ]
  obs_i <- obs_i[!is.na(obs_i)]
  time_seq_obs_i <- time_seq[seq_len(length(obs_i))]
  time_seq_obs_i_normalized <- (time_seq_obs_i / max(time_seq_obs_i)) * 100
  fdsmooth_i <- smooth.basis(argvals = time_seq_obs_i_normalized,
                             y = obs_i, 
                             fdParobj = Bspline_basis_k50_normalized)
  coef_matrix[i, ] <- fdsmooth_i$fd$coefs
}

# create fd object
GRF_fd_2 <- fd(coef = t(coef_matrix), basisobj = Bspline_basis_k50_normalized)

par(mfrow = c(1, 2))
plot(GRF_fd)
title("Approach 1")
plot(GRF_fd_2)
title("Approach 2")
```


* [Link to flowchart from my thesis](https://github.com/edwardgunning/thesis-chapt-3/blob/main/outputs/basis-expansion-diagram/2023-01-02-basis-expansion-diagram.pdf).

#### <u>Other approaches and further thoughts</u>

* If you time normalise, you should carry through curve length as an additional parameter. Don't just discard it!

* There are some other approaches people have considered. They don't make sense in this context (I think), but I'm demonstrating them just to show that *how you define your domain* matters.

* In some instances, possibly where the data are not periodic/ repeating, it may make more sense to pad the data with the final value (sometimes zero for forces data).

```{r}
GRF_matrix_zero_padded <- GRF_matrix
GRF_matrix_zero_padded[is.na(GRF_matrix_zero_padded)] <- 0
```

* Or, you might choose a physical time domain (un-normalized) over which all the curves are defined.

```{r}
min_time <- min(apply(GRF_matrix, 1, function(x) sum(is.na(x))))
time_seq_min <- time_seq[seq_len(min_time)]
GRF_matrix_min <- GRF_matrix[, seq_len(min_time)]
```

* Let's have a look at all three approaches and the original data side-by-side.

```{r, fig.asp=1, fig.width=8}
par(mfrow = c(2, 2))
matplot(time_seq, t(GRF_matrix),  
        type = "b", 
        cex = 0.5, 
        pch = 20)
title("Raw Data")
matplot(time_seq_normalized, t(GRF_matrix_normalized),
        type = "b", 
        cex = 0.5, 
        pch = 20)
title("Time Normalised")
matplot(time_seq, t(GRF_matrix_zero_padded),  
        type = "b", 
        cex = 0.5, 
        pch = 20)
title("Zero-Padded")
matplot(time_seq_min, 
        t(GRF_matrix_min),  
        type = "b", 
        cex = 0.5, 
        pch = 20)
title("Chopped")
```

</details>

## Data Export

<details>

<summary>üñ±Ô∏è Click to Expand üñ± </summary>

* We typically want to save iterations of our data at multiple stages in the workflow, and will rarely complete an analysis in a single interactive session.

* Therefore, we need to be able to save and import the objects that we create.

* The canonical way to store R objects, such as the `fd` object we have created, is in "R‚Äôs custom binary format called RDS" (Wickham and Grolemund, 2017).

* The functions to save and read RDS objects are straightforward:

```{r}
# save 
saveRDS(object = GRF_fd, file = "saved-objects/GRF_fd.rds")
# and read back in
GRF_fd <- readRDS(file = "saved-objects/GRF_fd.rds")
```

* We could also consider svaing just the basis as an `RDS` object and the coefficients in a more familiar CSV or TSV format (e.g., using `write.csv()`).

</details>

## Summarizing the Data (basics)

<details>

<summary>üñ±Ô∏è Click to Expand üñ± </summary>

Once we have represented our data as an `fd` object, we can apply visualisation, summary and analysis functions from the `fda` package. Here are a number of simple visualisation functions that act directly on the `fd` object.

1. **`plot.fd()`** Plot the full functional data object.

```{r}
plot.fd(GRF_fd)
```

2. **`mean.fd()`** Calculate the sample mean of a functional data object (we'll then plot it).

```{r}
GRF_mean_fd <- mean.fd(GRF_fd)
plot.fd(GRF_mean_fd)
```

3. **`boxplot.fd`** Display the functional version of the boxplot Sun and Genton (2011) of the `fd` object (see  and Chapter 3 of our book for a summary of how to interpret it).

```{r}
boxplot.fd(GRF_fd)
```

</details>

## Conclusion (and some personal perspectives)

<details>

<summary>üñ±Ô∏è Click to Expand üñ± </summary>

* Biomechanical data is rich and structured.

* How we process it matters and affects our results and interpretations of an analysis.

* There is no "flowchart" or rigid set of steps for FDA. Thought needs to be given to context and intricacies of each dataset.

* Sensitivity analyses are often useful to determine the effects of our subjective choices.

* We hope that this tutorial has benefitted you, and we would love to hear about the data preparation issues that you encounter in your own work.


</details>

## üìñ Further Reading

* Gellar, Jonathan E., Elizabeth Colantuoni, Dale M. Needham, and Ciprian M. Crainiceanu. ‚ÄúVariable-Domain Functional Regression for Modeling ICU Data.‚Äù Journal of the American Statistical Association 109, no. 508 (2014): 1425‚Äì39. https://doi.org/10.1080/01621459.2014.940044.

* Sangalli, Laura M., Piercesare Secchi, Simone Vantini, and Valeria Vitelli. ‚ÄúK-Mean Alignment for Curve Clustering.‚Äù Computational Statistics & Data Analysis 54, no. 5 (2010): 1219‚Äì33. https://doi.org/10.1016/j.csda.2009.12.008.

* Sun, Ying, and Marc G. Genton. ‚ÄúFunctional Boxplots.‚Äù Journal of Computational and Graphical Statistics 20, no. 2 (2011): 316‚Äì34. https://doi.org/10.1198/jcgs.2011.09224.

* Ramsay J (2024). _fda: Functional Data Analysis_. R package version 6.1.8,
  <https://CRAN.R-project.org/package=fda>.

* Wickham, H., & Grolemund, G. (2017). R for data science (Vol. 2). Sebastopol, CA: O'Reilly.



## Session Info for Reproducibility

```{r}
sessionInfo()
```


